{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6578c72c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9870c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.rnn import LeadIILSTM, LSTM_12Leads\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from neurokit2 import ecg\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SAMPLING_RATE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edff6a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "if (!(\"Notification\" in window)) {\n    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n    Notification.requestPermission(function (permission) {\n        if(!('permission' in Notification)) {\n            Notification.permission = permission;\n        }\n    })\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c29f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signals shape: torch.Size([8, 12, 10000])\n",
      "Labels: tensor([2, 3, 0, 2, 2, 1, 0, 2])\n",
      "ECG IDs: ('270369.parquet.gzip', '533892.parquet.gzip', '399764.parquet.gzip', '375116.parquet.gzip', '52639.parquet.gzip', '349968.parquet.gzip', '495783.parquet.gzip', '320776.parquet.gzip')\n"
     ]
    }
   ],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, data_folder, class_folders, files_per_class=200):\n",
    "        self.samples = []\n",
    "        self.leads = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "        for folder, label in class_folders.items():\n",
    "            files = glob.glob(os.path.join(data_folder, folder, '*.parquet.gzip'))\n",
    "            # enforce exact files_per_class per class (downsample or upsample with replacement)\n",
    "            if len(files) >= files_per_class:\n",
    "                files = random.sample(files, files_per_class)\n",
    "            else:\n",
    "                files = random.choices(files, k=files_per_class)\n",
    "\n",
    "            for f in files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(f, engine='fastparquet')\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read {f}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # ensure required lead columns exist\n",
    "                if not set(self.leads).issubset(df.columns):\n",
    "                    print(f\"Missing leads in {f}, skipping\")\n",
    "                    continue\n",
    "\n",
    "                # convert lead columns to numeric, coerce non-numeric to NaN, then fill and cast\n",
    "                df_leads = df[self.leads].apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n",
    "\n",
    "                # shape -> (12, time)\n",
    "                signal = df_leads.values.T\n",
    "                self.samples.append((torch.tensor(signal, dtype=torch.float32), label, os.path.basename(f)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal, label, ecg_id = self.samples[idx]\n",
    "        return signal, label, ecg_id\n",
    "\n",
    "# Usage example\n",
    "class_folders = {\n",
    "    'arritmia': 0,\n",
    "    'block': 1,\n",
    "    'fibrilation': 2,\n",
    "    'normal': 3\n",
    "}\n",
    "data_folder = 'data'\n",
    "dataset = ECGDataset(data_folder, class_folders, files_per_class=1970)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "for signals, labels, ecg_ids in dataloader:\n",
    "    print('Signals shape:', signals.shape)  # (batch, 12, time)\n",
    "    print('Labels:', labels)\n",
    "    print('ECG IDs:', ecg_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f62b31",
   "metadata": {},
   "source": [
    "ECG eliminados por peso (2kb):\n",
    "\n",
    "- block: 8846, 314864\n",
    "- normal: 74424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c05b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare DataLoader for 12-lead LSTM with downsamping to fixed length ---\n",
    "# Original signals are (12, time). We'll resample each to `target_len` (e.g., 300) to keep memory and compute reasonable.\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "target_len = 300  # desired sequence length after downsampling (try 200-300)\n",
    "\n",
    "def resample_signal(sig, target_len):\n",
    "    # sig: Tensor shape (12, time)\n",
    "    # convert to (1, channels, length) for F.interpolate -> (1,12,L)\n",
    "    x = sig.unsqueeze(0)  # (1, 12, time)\n",
    "    x_res = F.interpolate(x, size=target_len, mode='linear', align_corners=False)\n",
    "    x_res = x_res.squeeze(0)  # (12, target_len)\n",
    "    # return as (target_len, 12)\n",
    "    return x_res.permute(1, 0)\n",
    "\n",
    "def collate_ecg(batch):\n",
    "    # batch: list of (signal_tensor (12, time), label, ecg_id)\n",
    "    resampled = [resample_signal(item[0], target_len) for item in batch]  # list of (target_len, 12)\n",
    "    # stack -> (batch, target_len, 12)\n",
    "    batch_tensor = torch.stack(resampled, dim=0)\n",
    "    labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "    ids = [item[2] for item in batch]\n",
    "    return batch_tensor, labels, ids\n",
    "\n",
    "# Create a smaller dataset/dataloader for a quick demo (avoid using all files during tests)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_ecg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e47902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train_loss: 1.2571 - train_acc: 0.4132\n",
      "          val_acc: 0.4467\n",
      "          val_acc: 0.4467\n",
      "Epoch 2 - train_loss: 1.1747 - train_acc: 0.4659\n",
      "Epoch 2 - train_loss: 1.1747 - train_acc: 0.4659\n",
      "          val_acc: 0.4784\n",
      "          val_acc: 0.4784\n",
      "Epoch 3 - train_loss: 1.1391 - train_acc: 0.4860\n",
      "Epoch 3 - train_loss: 1.1391 - train_acc: 0.4860\n",
      "          val_acc: 0.4860\n",
      "          val_acc: 0.4860\n",
      "Epoch 4 - train_loss: 1.1139 - train_acc: 0.4957\n",
      "Epoch 4 - train_loss: 1.1139 - train_acc: 0.4957\n",
      "          val_acc: 0.4860\n",
      "          val_acc: 0.4860\n",
      "Epoch 5 - train_loss: 1.1031 - train_acc: 0.5060\n",
      "Epoch 5 - train_loss: 1.1031 - train_acc: 0.5060\n",
      "          val_acc: 0.4829\n",
      "          val_acc: 0.4829\n",
      "Epoch 6 - train_loss: 1.0846 - train_acc: 0.5138\n",
      "Epoch 6 - train_loss: 1.0846 - train_acc: 0.5138\n",
      "          val_acc: 0.5006\n",
      "          val_acc: 0.5006\n",
      "Epoch 7 - train_loss: 1.0622 - train_acc: 0.5274\n",
      "Epoch 7 - train_loss: 1.0622 - train_acc: 0.5274\n",
      "          val_acc: 0.5076\n",
      "          val_acc: 0.5076\n",
      "Epoch 8 - train_loss: 1.0447 - train_acc: 0.5378\n",
      "Epoch 8 - train_loss: 1.0447 - train_acc: 0.5378\n",
      "          val_acc: 0.5108\n",
      "          val_acc: 0.5108\n",
      "Epoch 9 - train_loss: 1.0646 - train_acc: 0.5320\n",
      "Epoch 9 - train_loss: 1.0646 - train_acc: 0.5320\n",
      "          val_acc: 0.5140\n",
      "          val_acc: 0.5140\n",
      "Epoch 10 - train_loss: 1.0459 - train_acc: 0.5428\n",
      "Epoch 10 - train_loss: 1.0459 - train_acc: 0.5428\n",
      "          val_acc: 0.5216\n",
      "          val_acc: 0.5216\n",
      "Epoch 11 - train_loss: 1.0376 - train_acc: 0.5466\n",
      "Epoch 11 - train_loss: 1.0376 - train_acc: 0.5466\n",
      "          val_acc: 0.5159\n",
      "          val_acc: 0.5159\n",
      "Epoch 12 - train_loss: 1.0199 - train_acc: 0.5562\n",
      "Epoch 12 - train_loss: 1.0199 - train_acc: 0.5562\n",
      "          val_acc: 0.5317\n",
      "          val_acc: 0.5317\n",
      "Epoch 13 - train_loss: 1.0314 - train_acc: 0.5512\n",
      "Epoch 13 - train_loss: 1.0314 - train_acc: 0.5512\n",
      "          val_acc: 0.5273\n",
      "          val_acc: 0.5273\n",
      "Epoch 14 - train_loss: 1.0161 - train_acc: 0.5614\n",
      "Epoch 14 - train_loss: 1.0161 - train_acc: 0.5614\n",
      "          val_acc: 0.5286\n",
      "          val_acc: 0.5286\n",
      "Epoch 15 - train_loss: 1.0038 - train_acc: 0.5682\n",
      "Epoch 15 - train_loss: 1.0038 - train_acc: 0.5682\n",
      "          val_acc: 0.5095\n",
      "          val_acc: 0.5095\n",
      "Epoch 16 - train_loss: 0.9997 - train_acc: 0.5657\n",
      "Epoch 16 - train_loss: 0.9997 - train_acc: 0.5657\n",
      "          val_acc: 0.5349\n",
      "          val_acc: 0.5349\n",
      "Epoch 17 - train_loss: 1.0094 - train_acc: 0.5665\n",
      "Epoch 17 - train_loss: 1.0094 - train_acc: 0.5665\n",
      "          val_acc: 0.5279\n",
      "          val_acc: 0.5279\n",
      "Epoch 18 - train_loss: 0.9957 - train_acc: 0.5688\n",
      "Epoch 18 - train_loss: 0.9957 - train_acc: 0.5688\n",
      "          val_acc: 0.5374\n",
      "          val_acc: 0.5374\n",
      "Epoch 19 - train_loss: 0.9946 - train_acc: 0.5668\n",
      "Epoch 19 - train_loss: 0.9946 - train_acc: 0.5668\n",
      "          val_acc: 0.5438\n",
      "          val_acc: 0.5438\n",
      "Epoch 20 - train_loss: 0.9931 - train_acc: 0.5712\n",
      "Epoch 20 - train_loss: 0.9931 - train_acc: 0.5712\n",
      "          val_acc: 0.5451\n",
      "          val_acc: 0.5451\n",
      "Epoch 21 - train_loss: 0.9873 - train_acc: 0.5814\n",
      "Epoch 21 - train_loss: 0.9873 - train_acc: 0.5814\n",
      "          val_acc: 0.5241\n",
      "          val_acc: 0.5241\n",
      "Epoch 22 - train_loss: 0.9754 - train_acc: 0.5782\n",
      "Epoch 22 - train_loss: 0.9754 - train_acc: 0.5782\n",
      "          val_acc: 0.5571\n",
      "          val_acc: 0.5571\n",
      "Epoch 23 - train_loss: 0.9662 - train_acc: 0.5887\n",
      "Epoch 23 - train_loss: 0.9662 - train_acc: 0.5887\n",
      "          val_acc: 0.5501\n",
      "          val_acc: 0.5501\n",
      "Epoch 24 - train_loss: 0.9718 - train_acc: 0.5807\n",
      "Epoch 24 - train_loss: 0.9718 - train_acc: 0.5807\n",
      "          val_acc: 0.5412\n",
      "          val_acc: 0.5412\n",
      "Epoch 25 - train_loss: 0.9670 - train_acc: 0.5819\n",
      "Epoch 25 - train_loss: 0.9670 - train_acc: 0.5819\n",
      "          val_acc: 0.5330\n",
      "          val_acc: 0.5330\n",
      "Epoch 26 - train_loss: 0.9613 - train_acc: 0.5876\n",
      "Epoch 26 - train_loss: 0.9613 - train_acc: 0.5876\n",
      "          val_acc: 0.5349\n",
      "          val_acc: 0.5349\n",
      "Epoch 27 - train_loss: 0.9752 - train_acc: 0.5769\n",
      "Epoch 27 - train_loss: 0.9752 - train_acc: 0.5769\n",
      "          val_acc: 0.5292\n",
      "          val_acc: 0.5292\n",
      "Epoch 28 - train_loss: 0.9605 - train_acc: 0.5880\n",
      "Epoch 28 - train_loss: 0.9605 - train_acc: 0.5880\n",
      "          val_acc: 0.5622\n",
      "          val_acc: 0.5622\n",
      "Epoch 29 - train_loss: 0.9524 - train_acc: 0.5934\n",
      "Epoch 29 - train_loss: 0.9524 - train_acc: 0.5934\n",
      "          val_acc: 0.5355\n",
      "          val_acc: 0.5355\n",
      "Epoch 30 - train_loss: 0.9555 - train_acc: 0.5977\n",
      "Epoch 30 - train_loss: 0.9555 - train_acc: 0.5977\n",
      "          val_acc: 0.5400\n",
      "          val_acc: 0.5400\n",
      "Epoch 31 - train_loss: 0.9589 - train_acc: 0.5876\n",
      "Epoch 31 - train_loss: 0.9589 - train_acc: 0.5876\n",
      "          val_acc: 0.5393\n",
      "          val_acc: 0.5393\n",
      "Epoch 32 - train_loss: 0.9587 - train_acc: 0.5938\n",
      "Epoch 32 - train_loss: 0.9587 - train_acc: 0.5938\n",
      "          val_acc: 0.5451\n",
      "          val_acc: 0.5451\n",
      "Epoch 33 - train_loss: 0.9519 - train_acc: 0.5995\n",
      "Epoch 33 - train_loss: 0.9519 - train_acc: 0.5995\n",
      "          val_acc: 0.5444\n",
      "          val_acc: 0.5444\n",
      "Epoch 34 - train_loss: 0.9490 - train_acc: 0.5976\n",
      "Epoch 34 - train_loss: 0.9490 - train_acc: 0.5976\n",
      "          val_acc: 0.5209\n",
      "          val_acc: 0.5209\n",
      "Epoch 35 - train_loss: 0.9386 - train_acc: 0.6033\n",
      "Epoch 35 - train_loss: 0.9386 - train_acc: 0.6033\n",
      "          val_acc: 0.5438\n",
      "          val_acc: 0.5438\n",
      "Epoch 36 - train_loss: 0.9307 - train_acc: 0.6082\n",
      "Epoch 36 - train_loss: 0.9307 - train_acc: 0.6082\n",
      "          val_acc: 0.5451\n",
      "          val_acc: 0.5451\n",
      "Epoch 37 - train_loss: 0.9303 - train_acc: 0.5982\n",
      "Epoch 37 - train_loss: 0.9303 - train_acc: 0.5982\n",
      "          val_acc: 0.5368\n",
      "          val_acc: 0.5368\n",
      "Epoch 38 - train_loss: 0.9292 - train_acc: 0.6055\n",
      "Epoch 38 - train_loss: 0.9292 - train_acc: 0.6055\n",
      "          val_acc: 0.5362\n",
      "          val_acc: 0.5362\n",
      "Epoch 39 - train_loss: 0.9232 - train_acc: 0.6117\n",
      "Epoch 39 - train_loss: 0.9232 - train_acc: 0.6117\n",
      "          val_acc: 0.5298\n",
      "          val_acc: 0.5298\n",
      "Epoch 40 - train_loss: 0.9366 - train_acc: 0.6033\n",
      "Epoch 40 - train_loss: 0.9366 - train_acc: 0.6033\n",
      "          val_acc: 0.5533\n",
      "          val_acc: 0.5533\n",
      "Epoch 41 - train_loss: 0.9175 - train_acc: 0.6064\n",
      "Epoch 41 - train_loss: 0.9175 - train_acc: 0.6064\n",
      "          val_acc: 0.5514\n",
      "          val_acc: 0.5514\n",
      "Epoch 42 - train_loss: 0.9160 - train_acc: 0.6137\n",
      "Epoch 42 - train_loss: 0.9160 - train_acc: 0.6137\n",
      "          val_acc: 0.5457\n",
      "          val_acc: 0.5457\n",
      "Epoch 43 - train_loss: 0.9106 - train_acc: 0.6118\n",
      "Epoch 43 - train_loss: 0.9106 - train_acc: 0.6118\n",
      "          val_acc: 0.5514\n",
      "          val_acc: 0.5514\n",
      "Epoch 44 - train_loss: 0.9125 - train_acc: 0.6177\n",
      "Epoch 44 - train_loss: 0.9125 - train_acc: 0.6177\n",
      "          val_acc: 0.5552\n",
      "          val_acc: 0.5552\n",
      "Epoch 45 - train_loss: 0.9252 - train_acc: 0.6112\n",
      "Epoch 45 - train_loss: 0.9252 - train_acc: 0.6112\n",
      "          val_acc: 0.5165\n",
      "          val_acc: 0.5165\n",
      "Epoch 46 - train_loss: 0.9169 - train_acc: 0.6102\n",
      "Epoch 46 - train_loss: 0.9169 - train_acc: 0.6102\n",
      "          val_acc: 0.5463\n",
      "          val_acc: 0.5463\n",
      "Epoch 47 - train_loss: 0.8964 - train_acc: 0.6104\n",
      "Epoch 47 - train_loss: 0.8964 - train_acc: 0.6104\n",
      "          val_acc: 0.5647\n",
      "          val_acc: 0.5647\n",
      "Epoch 48 - train_loss: 0.8953 - train_acc: 0.6163\n",
      "Epoch 48 - train_loss: 0.8953 - train_acc: 0.6163\n",
      "          val_acc: 0.5622\n",
      "          val_acc: 0.5622\n",
      "Epoch 49 - train_loss: 0.8887 - train_acc: 0.6260\n",
      "Epoch 49 - train_loss: 0.8887 - train_acc: 0.6260\n",
      "          val_acc: 0.5628\n",
      "          val_acc: 0.5628\n",
      "Epoch 50 - train_loss: 0.8881 - train_acc: 0.6244\n",
      "Epoch 50 - train_loss: 0.8881 - train_acc: 0.6244\n",
      "          val_acc: 0.5546\n",
      "          val_acc: 0.5546\n",
      "Epoch 51 - train_loss: 0.8783 - train_acc: 0.6374\n",
      "Epoch 51 - train_loss: 0.8783 - train_acc: 0.6374\n",
      "          val_acc: 0.5463\n",
      "          val_acc: 0.5463\n",
      "Epoch 52 - train_loss: 0.8777 - train_acc: 0.6345\n",
      "Epoch 52 - train_loss: 0.8777 - train_acc: 0.6345\n",
      "          val_acc: 0.5825\n",
      "          val_acc: 0.5825\n",
      "Epoch 53 - train_loss: 0.8776 - train_acc: 0.6329\n",
      "Epoch 53 - train_loss: 0.8776 - train_acc: 0.6329\n",
      "          val_acc: 0.5628\n",
      "          val_acc: 0.5628\n",
      "Epoch 54 - train_loss: 0.8739 - train_acc: 0.6337\n",
      "Epoch 54 - train_loss: 0.8739 - train_acc: 0.6337\n",
      "          val_acc: 0.5470\n",
      "          val_acc: 0.5470\n",
      "Epoch 55 - train_loss: 0.8752 - train_acc: 0.6328\n",
      "Epoch 55 - train_loss: 0.8752 - train_acc: 0.6328\n",
      "          val_acc: 0.5533\n",
      "          val_acc: 0.5533\n",
      "Epoch 56 - train_loss: 0.8729 - train_acc: 0.6345\n",
      "Epoch 56 - train_loss: 0.8729 - train_acc: 0.6345\n",
      "          val_acc: 0.5698\n",
      "          val_acc: 0.5698\n",
      "Epoch 57 - train_loss: 0.8617 - train_acc: 0.6399\n",
      "Epoch 57 - train_loss: 0.8617 - train_acc: 0.6399\n",
      "          val_acc: 0.5742\n",
      "          val_acc: 0.5742\n",
      "Epoch 58 - train_loss: 0.8581 - train_acc: 0.6356\n",
      "Epoch 58 - train_loss: 0.8581 - train_acc: 0.6356\n",
      "          val_acc: 0.5571\n",
      "          val_acc: 0.5571\n",
      "Epoch 59 - train_loss: 0.8609 - train_acc: 0.6350\n",
      "Epoch 59 - train_loss: 0.8609 - train_acc: 0.6350\n",
      "          val_acc: 0.5647\n",
      "          val_acc: 0.5647\n",
      "Epoch 60 - train_loss: 0.8629 - train_acc: 0.6345\n",
      "Epoch 60 - train_loss: 0.8629 - train_acc: 0.6345\n",
      "          val_acc: 0.5584\n",
      "          val_acc: 0.5584\n",
      "Epoch 61 - train_loss: 0.8594 - train_acc: 0.6361\n",
      "Epoch 61 - train_loss: 0.8594 - train_acc: 0.6361\n",
      "          val_acc: 0.5387\n",
      "          val_acc: 0.5387\n",
      "Epoch 62 - train_loss: 0.8534 - train_acc: 0.6375\n",
      "Epoch 62 - train_loss: 0.8534 - train_acc: 0.6375\n",
      "          val_acc: 0.5692\n",
      "          val_acc: 0.5692\n",
      "Epoch 63 - train_loss: 0.8593 - train_acc: 0.6361\n",
      "Epoch 63 - train_loss: 0.8593 - train_acc: 0.6361\n",
      "          val_acc: 0.5615\n",
      "          val_acc: 0.5615\n",
      "Epoch 64 - train_loss: 0.8637 - train_acc: 0.6317\n",
      "Epoch 64 - train_loss: 0.8637 - train_acc: 0.6317\n",
      "          val_acc: 0.5749\n",
      "          val_acc: 0.5749\n",
      "Epoch 65 - train_loss: 0.8475 - train_acc: 0.6401\n",
      "Epoch 65 - train_loss: 0.8475 - train_acc: 0.6401\n",
      "          val_acc: 0.5590\n",
      "          val_acc: 0.5590\n",
      "Epoch 66 - train_loss: 0.8734 - train_acc: 0.6326\n",
      "Epoch 66 - train_loss: 0.8734 - train_acc: 0.6326\n",
      "          val_acc: 0.5349\n",
      "          val_acc: 0.5349\n",
      "Epoch 67 - train_loss: 0.8744 - train_acc: 0.6348\n",
      "Epoch 67 - train_loss: 0.8744 - train_acc: 0.6348\n",
      "          val_acc: 0.5406\n",
      "          val_acc: 0.5406\n",
      "Epoch 68 - train_loss: 0.8733 - train_acc: 0.6382\n",
      "Epoch 68 - train_loss: 0.8733 - train_acc: 0.6382\n",
      "          val_acc: 0.5628\n",
      "          val_acc: 0.5628\n",
      "Epoch 69 - train_loss: 0.8587 - train_acc: 0.6410\n",
      "Epoch 69 - train_loss: 0.8587 - train_acc: 0.6410\n",
      "          val_acc: 0.5761\n",
      "          val_acc: 0.5761\n",
      "Epoch 70 - train_loss: 0.8462 - train_acc: 0.6478\n",
      "Epoch 70 - train_loss: 0.8462 - train_acc: 0.6478\n",
      "          val_acc: 0.5590\n",
      "          val_acc: 0.5590\n",
      "Epoch 71 - train_loss: 0.8375 - train_acc: 0.6466\n",
      "Epoch 71 - train_loss: 0.8375 - train_acc: 0.6466\n",
      "          val_acc: 0.5527\n",
      "          val_acc: 0.5527\n",
      "Epoch 72 - train_loss: 0.8379 - train_acc: 0.6494\n",
      "Epoch 72 - train_loss: 0.8379 - train_acc: 0.6494\n",
      "          val_acc: 0.5761\n",
      "          val_acc: 0.5761\n",
      "Epoch 73 - train_loss: 0.8457 - train_acc: 0.6469\n",
      "Epoch 73 - train_loss: 0.8457 - train_acc: 0.6469\n",
      "          val_acc: 0.5647\n",
      "          val_acc: 0.5647\n",
      "Epoch 74 - train_loss: 0.8392 - train_acc: 0.6510\n",
      "Epoch 74 - train_loss: 0.8392 - train_acc: 0.6510\n",
      "          val_acc: 0.5539\n",
      "          val_acc: 0.5539\n",
      "Epoch 75 - train_loss: 0.8528 - train_acc: 0.6410\n",
      "Epoch 75 - train_loss: 0.8528 - train_acc: 0.6410\n",
      "          val_acc: 0.5704\n",
      "          val_acc: 0.5704\n",
      "Epoch 76 - train_loss: 0.8527 - train_acc: 0.6458\n",
      "Epoch 76 - train_loss: 0.8527 - train_acc: 0.6458\n",
      "          val_acc: 0.5584\n",
      "          val_acc: 0.5584\n",
      "Epoch 77 - train_loss: 0.8371 - train_acc: 0.6532\n",
      "Epoch 77 - train_loss: 0.8371 - train_acc: 0.6532\n",
      "          val_acc: 0.5685\n",
      "          val_acc: 0.5685\n",
      "Epoch 78 - train_loss: 0.8356 - train_acc: 0.6545\n",
      "Epoch 78 - train_loss: 0.8356 - train_acc: 0.6545\n",
      "          val_acc: 0.5577\n",
      "          val_acc: 0.5577\n",
      "Epoch 79 - train_loss: 0.8395 - train_acc: 0.6545\n",
      "Epoch 79 - train_loss: 0.8395 - train_acc: 0.6545\n",
      "          val_acc: 0.5812\n",
      "          val_acc: 0.5812\n",
      "Epoch 80 - train_loss: 0.8521 - train_acc: 0.6448\n",
      "Epoch 80 - train_loss: 0.8521 - train_acc: 0.6448\n",
      "          val_acc: 0.5584\n",
      "          val_acc: 0.5584\n",
      "Epoch 81 - train_loss: 0.8575 - train_acc: 0.6394\n",
      "Epoch 81 - train_loss: 0.8575 - train_acc: 0.6394\n",
      "          val_acc: 0.5533\n",
      "          val_acc: 0.5533\n",
      "Epoch 82 - train_loss: 0.8523 - train_acc: 0.6436\n",
      "Epoch 82 - train_loss: 0.8523 - train_acc: 0.6436\n",
      "          val_acc: 0.5527\n",
      "          val_acc: 0.5527\n",
      "Epoch 83 - train_loss: 0.8466 - train_acc: 0.6464\n",
      "Epoch 83 - train_loss: 0.8466 - train_acc: 0.6464\n",
      "          val_acc: 0.5546\n",
      "          val_acc: 0.5546\n",
      "Epoch 84 - train_loss: 0.8471 - train_acc: 0.6413\n",
      "Epoch 84 - train_loss: 0.8471 - train_acc: 0.6413\n",
      "          val_acc: 0.5622\n",
      "          val_acc: 0.5622\n",
      "Epoch 85 - train_loss: 0.8362 - train_acc: 0.6548\n",
      "Epoch 85 - train_loss: 0.8362 - train_acc: 0.6548\n",
      "          val_acc: 0.5463\n",
      "          val_acc: 0.5463\n",
      "Epoch 86 - train_loss: 0.8313 - train_acc: 0.6509\n",
      "Epoch 86 - train_loss: 0.8313 - train_acc: 0.6509\n",
      "          val_acc: 0.5520\n",
      "          val_acc: 0.5520\n",
      "Epoch 87 - train_loss: 0.8425 - train_acc: 0.6477\n",
      "Epoch 87 - train_loss: 0.8425 - train_acc: 0.6477\n",
      "          val_acc: 0.5654\n",
      "          val_acc: 0.5654\n",
      "Epoch 88 - train_loss: 0.8239 - train_acc: 0.6575\n",
      "Epoch 88 - train_loss: 0.8239 - train_acc: 0.6575\n",
      "          val_acc: 0.5374\n",
      "          val_acc: 0.5374\n",
      "Epoch 89 - train_loss: 0.8316 - train_acc: 0.6532\n",
      "Epoch 89 - train_loss: 0.8316 - train_acc: 0.6532\n",
      "          val_acc: 0.5577\n",
      "          val_acc: 0.5577\n",
      "Epoch 90 - train_loss: 0.8169 - train_acc: 0.6645\n",
      "Epoch 90 - train_loss: 0.8169 - train_acc: 0.6645\n",
      "          val_acc: 0.5615\n",
      "          val_acc: 0.5615\n",
      "Epoch 91 - train_loss: 0.8066 - train_acc: 0.6602\n",
      "Epoch 91 - train_loss: 0.8066 - train_acc: 0.6602\n",
      "          val_acc: 0.5362\n",
      "          val_acc: 0.5362\n",
      "Epoch 92 - train_loss: 0.8006 - train_acc: 0.6739\n",
      "Epoch 92 - train_loss: 0.8006 - train_acc: 0.6739\n",
      "          val_acc: 0.5584\n",
      "          val_acc: 0.5584\n",
      "Epoch 93 - train_loss: 0.8220 - train_acc: 0.6613\n",
      "Epoch 93 - train_loss: 0.8220 - train_acc: 0.6613\n",
      "          val_acc: 0.5216\n",
      "          val_acc: 0.5216\n",
      "Epoch 94 - train_loss: 0.8204 - train_acc: 0.6618\n",
      "Epoch 94 - train_loss: 0.8204 - train_acc: 0.6618\n",
      "          val_acc: 0.5539\n",
      "          val_acc: 0.5539\n",
      "Epoch 95 - train_loss: 0.8151 - train_acc: 0.6631\n",
      "Epoch 95 - train_loss: 0.8151 - train_acc: 0.6631\n",
      "          val_acc: 0.5615\n",
      "          val_acc: 0.5615\n",
      "Epoch 96 - train_loss: 0.8084 - train_acc: 0.6637\n",
      "Epoch 96 - train_loss: 0.8084 - train_acc: 0.6637\n",
      "          val_acc: 0.5451\n",
      "          val_acc: 0.5451\n",
      "Epoch 97 - train_loss: 0.8104 - train_acc: 0.6635\n",
      "Epoch 97 - train_loss: 0.8104 - train_acc: 0.6635\n",
      "          val_acc: 0.5489\n",
      "          val_acc: 0.5489\n",
      "Epoch 98 - train_loss: 0.8128 - train_acc: 0.6683\n",
      "Epoch 98 - train_loss: 0.8128 - train_acc: 0.6683\n",
      "          val_acc: 0.5260\n",
      "          val_acc: 0.5260\n",
      "Epoch 99 - train_loss: 0.8161 - train_acc: 0.6647\n",
      "Epoch 99 - train_loss: 0.8161 - train_acc: 0.6647\n",
      "          val_acc: 0.5539\n",
      "          val_acc: 0.5539\n",
      "Epoch 100 - train_loss: 0.8055 - train_acc: 0.6650\n",
      "Epoch 100 - train_loss: 0.8055 - train_acc: 0.6650\n",
      "          val_acc: 0.5406\n",
      "          val_acc: 0.5406\n"
     ]
    },
    {
     "data": {
      "application/javascript": "$(document).ready(\n    function() {\n        function appendUniqueDiv(){\n            // append a div with our uuid so we can check that it's already\n            // been sent and avoid duplicates on page reload\n            var notifiedDiv = document.createElement(\"div\")\n            notifiedDiv.id = \"2d343f1e-0c72-476a-b049-11d05fffa5e5\"\n            element.append(notifiedDiv)\n        }\n\n        // only send notifications if the pageload is complete; this will\n        // help stop extra notifications when a saved notebook is loaded,\n        // which during testing gives us state \"interactive\", not \"complete\"\n        if (document.readyState === 'complete') {\n            // check for the div that signifies that the notification\n            // was already sent\n            if (document.getElementById(\"2d343f1e-0c72-476a-b049-11d05fffa5e5\") === null) {\n                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n                if (Notification.permission !== 'denied') {\n                    if (Notification.permission !== 'granted') { \n                        Notification.requestPermission(function (permission) {\n                            if(!('permission' in Notification)) {\n                                Notification.permission = permission\n                            }\n                        })\n                    }\n                    if (Notification.permission === 'granted') {\n                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n                    appendUniqueDiv()\n                    notification.onclick = function () {\n                        window.focus();\n                        this.close();\n                        };\n                    } \n                }     \n            }\n        }\n    }\n)\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "indices = list(range(len(dataset)))\n",
    "labels_arr = [dataset.samples[i][1] for i in indices]  # extract labels for stratify\n",
    "\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=labels_arr, random_state=42)\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_ecg)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_ecg)\n",
    "\n",
    "# --- Instantiate model & optim (existing) ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM_12Leads(n_channels=12, hidden_size=64, num_layers=1, num_classes=4, bidirectional=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training with validation after each epoch\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for signals, labels, ids in train_loader:\n",
    "        signals = signals.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(signals)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * signals.size(0)\n",
    "        total += signals.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    train_acc = correct / total if total else 0.0\n",
    "    print(f'Epoch {epoch+1} - train_loss: {total_loss/total:.4f} - train_acc: {train_acc:.4f}')\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "    with torch.inference_mode():\n",
    "        for signals, labels, ids in val_loader:\n",
    "            signals = signals.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(signals)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_total += signals.size(0)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "    val_acc = val_correct / val_total if val_total else 0.0\n",
    "    print(f'          val_acc: {val_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
