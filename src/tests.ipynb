{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9870c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.rnn import CRNN_Model\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "from neurokit2 import ecg\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import stumpy\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SAMPLING_RATE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edff6a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saul/Documents/ECG/dl-ecg/.venv/lib/python3.13/site-packages/jupyternotify/jupyternotify.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5387c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signals shape: torch.Size([8, 12, 10000])\n",
      "Labels: tensor([1, 2, 1, 0, 0, 3, 0, 1])\n",
      "ECG IDs: ('329963.parquet.gzip', '439396.parquet.gzip', '350990.parquet.gzip', '404857.parquet.gzip', '541222.parquet.gzip', '295076.parquet.gzip', '339988.parquet.gzip', '523886.parquet.gzip')\n"
     ]
    }
   ],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, data_folder, class_folders, files_per_class=200, mp_window = 1000):\n",
    "        self.samples = []\n",
    "        self.leads = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "        self.mp_window = mp_window\n",
    "        for folder, label in class_folders.items():\n",
    "            files = glob.glob(os.path.join(data_folder, folder, '*.parquet.gzip'))\n",
    "            # enforce exact files_per_class per class (downsample or upsample with replacement)\n",
    "            if len(files) >= files_per_class:\n",
    "                files = random.sample(files, files_per_class)\n",
    "            else:\n",
    "                files = random.choices(files, k=files_per_class)\n",
    "\n",
    "            for f in files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(f, engine='fastparquet')\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read {f}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # ensure required lead columns exist\n",
    "                if not set(self.leads).issubset(df.columns):\n",
    "                    print(f\"Missing leads in {f}, skipping\")\n",
    "                    continue\n",
    "\n",
    "                # convert lead columns to numeric, coerce non-numeric to NaN, then fill and cast\n",
    "                df_leads = df[self.leads].apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n",
    "\n",
    "                # shape -> (12, time)\n",
    "                signal = df_leads.values.T\n",
    "\n",
    "                matrix_profiles = []\n",
    "                for i in range(signal.shape[0]):\n",
    "                    mp = stumpy.stump(signal[i].astype(np.float64), m=self.mp_window)[:, 0].astype(np.float32)\n",
    "                    pad_width = signal.shape[1] - len(mp)\n",
    "                    padded_mp = np.pad(mp, (0, pad_width), 'constant', constant_values=0)\n",
    "                    padded_mp[np.isinf(padded_mp)] = 1e9\n",
    "                    matrix_profiles.append(padded_mp)\n",
    "                matrix_profiles_np = np.array(matrix_profiles, dtype=np.float32)\n",
    "                combined_signal = np.concatenate((signal, matrix_profiles_np), axis=0)\n",
    "                # self.samples.append((torch.tensor(combined_signal, dtype=torch.float32), label, os.path.basename(f)))\n",
    "                self.samples.append((torch.tensor(matrix_profiles_np, dtype=torch.float32), label, os.path.basename(f)))\n",
    "                # self.samples.append((torch.tensor(signal, dtype=torch.float32), label, os.path.basename(f)))\n",
    "    def process_file(self, f, label):\n",
    "        try:\n",
    "            df = pd.read_parquet(f, engine='fastparquet')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {f}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        if not set(self.leads).issubset(df.columns):\n",
    "            print(f\"Missing leads in {f}, skipping\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal, label, ecg_id = self.samples[idx]\n",
    "        return signal, label, ecg_id\n",
    "\n",
    "# Usage example\n",
    "class_folders = {\n",
    "    'arritmia': 0,\n",
    "    'block': 1,\n",
    "    'fibrilation': 2,\n",
    "    'normal': 3\n",
    "}\n",
    "data_folder = 'data'\n",
    "dataset = ECGDataset(data_folder, class_folders, files_per_class=1970)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "for signals, labels, ecg_ids in dataloader:\n",
    "    print('Signals shape:', signals.shape)  # (batch, 12, time)\n",
    "    print('Labels:', labels)\n",
    "    print('ECG IDs:', ecg_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f7abe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 40 samples + split indices to loader12MP_CRNN.pth\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(len(dataset)))\n",
    "labels_arr = [dataset.samples[i][1] for i in indices]\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=labels_arr, random_state=42)\n",
    "\n",
    "serializable = {\n",
    "    'samples': [(sig.cpu().numpy(), int(label), str(ecg_id)) for sig, label, ecg_id in dataset.samples],\n",
    "    'train_idx': train_idx,\n",
    "    'val_idx': val_idx,\n",
    "    'leads': dataset.leads,\n",
    "    'mp_window': dataset.mp_window\n",
    "}\n",
    "torch.save(serializable, 'loader12MP_CRNN.pth')\n",
    "print(f\"Saved {len(serializable['samples'])} samples + split indices to loader12MP_CRNN.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97fbb47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data from loader12MP_CRNN.pth...\n",
      "Loaded 40 total samples.\n",
      "Training samples: 32, Validation samples: 8\n",
      "\n",
      "DataLoaders created successfully.\n",
      "Sample batch shape: torch.Size([8, 12, 10000])\n",
      "Sample batch labels: tensor([3, 3, 2, 2, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "class PreloadedECGDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        # Convert numpy arrays back to tensors\n",
    "        self.samples = [(torch.tensor(sig, dtype=torch.float32), label, ecg_id) for sig, label, ecg_id in samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# --- Load the pre-processed data ---\n",
    "print(\"Loading pre-processed data from loader12MP_CRNN.pth...\")\n",
    "loaded_data = torch.load('loader12MP_CRNN.pth', weights_only=False)\n",
    "\n",
    "# Extract the components\n",
    "all_samples = loaded_data['samples']\n",
    "train_idx = loaded_data['train_idx']\n",
    "val_idx = loaded_data['val_idx']\n",
    "\n",
    "print(f\"Loaded {len(all_samples)} total samples.\")\n",
    "print(f\"Training samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "# Create a full dataset object from the loaded samples\n",
    "full_dataset = PreloadedECGDataset(all_samples)\n",
    "\n",
    "# Create subsets for training and validation using the saved indices\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(full_dataset, val_idx)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"\\nDataLoaders created successfully.\")\n",
    "\n",
    "# Optional: Inspect a batch to verify\n",
    "signals, labels, ids = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {signals.shape}\")\n",
    "print(f\"Sample batch labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d5cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0, mode='min', checkpoint_path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.counter = 0\n",
    "        self.best_score = np.inf if mode == 'min' else -np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score, model):\n",
    "        is_better = False\n",
    "        if self.mode == 'min':\n",
    "            is_better = current_score < (self.best_score - self.min_delta)\n",
    "        else:\n",
    "            is_better = current_score > (self.best_score + self.min_delta)\n",
    "\n",
    "        if is_better:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            print(f\"Mejora detectada. Guardando modelo en {self.checkpoint_path}\")\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"Sin mejora. Contador de paciencia: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(\"--- EARLY STOPPING ACTIVADO ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f62b31",
   "metadata": {},
   "source": [
    "ECG eliminados por peso (2kb):\n",
    "\n",
    "- block: 8846, 314864\n",
    "- normal: 74424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e47902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando entrenamiento de CRNN en cuda ---\n",
      "Epoch 1/100 | Train Loss: 1.2504 - Train Acc: 0.3828 | Val Loss: 1.0820 - Val Acc: 0.4511\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 2/100 | Train Loss: 1.1076 - Train Acc: 0.4561 | Val Loss: 1.0540 - Val Acc: 0.4778\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 3/100 | Train Loss: 1.0362 - Train Acc: 0.4998 | Val Loss: 1.0191 - Val Acc: 0.4860\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 4/100 | Train Loss: 1.0070 - Train Acc: 0.5208 | Val Loss: 0.9682 - Val Acc: 0.5381\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 5/100 | Train Loss: 0.9760 - Train Acc: 0.5412 | Val Loss: 0.9698 - Val Acc: 0.5406\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 6/100 | Train Loss: 0.9712 - Train Acc: 0.5327 | Val Loss: 0.9499 - Val Acc: 0.5577\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 7/100 | Train Loss: 0.9625 - Train Acc: 0.5539 | Val Loss: 0.9403 - Val Acc: 0.5647\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 8/100 | Train Loss: 0.9467 - Train Acc: 0.5504 | Val Loss: 0.9320 - Val Acc: 0.5622\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 9/100 | Train Loss: 0.9329 - Train Acc: 0.5654 | Val Loss: 0.9404 - Val Acc: 0.5812\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 10/100 | Train Loss: 0.9347 - Train Acc: 0.5685 | Val Loss: 0.9286 - Val Acc: 0.5692\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 11/100 | Train Loss: 0.9251 - Train Acc: 0.5803 | Val Loss: 0.9174 - Val Acc: 0.5901\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 12/100 | Train Loss: 0.9076 - Train Acc: 0.6060 | Val Loss: 0.9003 - Val Acc: 0.5945\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 13/100 | Train Loss: 0.8764 - Train Acc: 0.6236 | Val Loss: 0.8800 - Val Acc: 0.6459\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 14/100 | Train Loss: 0.8514 - Train Acc: 0.6437 | Val Loss: 0.8647 - Val Acc: 0.6472\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 15/100 | Train Loss: 0.8347 - Train Acc: 0.6512 | Val Loss: 0.8130 - Val Acc: 0.6789\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 16/100 | Train Loss: 0.8030 - Train Acc: 0.6715 | Val Loss: 0.7894 - Val Acc: 0.6973\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 17/100 | Train Loss: 0.7975 - Train Acc: 0.6724 | Val Loss: 0.8114 - Val Acc: 0.6878\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 18/100 | Train Loss: 0.7752 - Train Acc: 0.6869 | Val Loss: 0.7797 - Val Acc: 0.6954\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 19/100 | Train Loss: 0.7620 - Train Acc: 0.6924 | Val Loss: 0.7872 - Val Acc: 0.6802\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 20/100 | Train Loss: 0.7436 - Train Acc: 0.7023 | Val Loss: 0.7934 - Val Acc: 0.6923\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 21/100 | Train Loss: 0.7460 - Train Acc: 0.7051 | Val Loss: 0.7589 - Val Acc: 0.7030\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 22/100 | Train Loss: 0.7279 - Train Acc: 0.7105 | Val Loss: 0.7654 - Val Acc: 0.6916\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 23/100 | Train Loss: 0.7243 - Train Acc: 0.7141 | Val Loss: 0.7502 - Val Acc: 0.7157\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 24/100 | Train Loss: 0.7226 - Train Acc: 0.7143 | Val Loss: 0.7271 - Val Acc: 0.7157\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 25/100 | Train Loss: 0.7168 - Train Acc: 0.7181 | Val Loss: 0.7261 - Val Acc: 0.7170\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 26/100 | Train Loss: 0.6995 - Train Acc: 0.7243 | Val Loss: 0.7108 - Val Acc: 0.7310\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 27/100 | Train Loss: 0.6977 - Train Acc: 0.7230 | Val Loss: 0.7157 - Val Acc: 0.7234\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 28/100 | Train Loss: 0.6898 - Train Acc: 0.7333 | Val Loss: 0.7170 - Val Acc: 0.7284\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 29/100 | Train Loss: 0.6830 - Train Acc: 0.7337 | Val Loss: 0.7325 - Val Acc: 0.7145\n",
      "Sin mejora. Contador de paciencia: 3 / 10\n",
      "Epoch 30/100 | Train Loss: 0.6928 - Train Acc: 0.7273 | Val Loss: 0.7211 - Val Acc: 0.7195\n",
      "Sin mejora. Contador de paciencia: 4 / 10\n",
      "Epoch 31/100 | Train Loss: 0.6720 - Train Acc: 0.7305 | Val Loss: 0.6914 - Val Acc: 0.7341\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 32/100 | Train Loss: 0.6807 - Train Acc: 0.7341 | Val Loss: 0.7027 - Val Acc: 0.7278\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 33/100 | Train Loss: 0.6648 - Train Acc: 0.7378 | Val Loss: 0.7030 - Val Acc: 0.7272\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 34/100 | Train Loss: 0.6814 - Train Acc: 0.7308 | Val Loss: 0.7014 - Val Acc: 0.7259\n",
      "Sin mejora. Contador de paciencia: 3 / 10\n",
      "Epoch 35/100 | Train Loss: 0.6649 - Train Acc: 0.7370 | Val Loss: 0.6884 - Val Acc: 0.7322\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 36/100 | Train Loss: 0.6480 - Train Acc: 0.7459 | Val Loss: 0.6907 - Val Acc: 0.7265\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 37/100 | Train Loss: 0.6438 - Train Acc: 0.7489 | Val Loss: 0.6943 - Val Acc: 0.7284\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 38/100 | Train Loss: 0.6421 - Train Acc: 0.7479 | Val Loss: 0.6744 - Val Acc: 0.7240\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 39/100 | Train Loss: 0.6290 - Train Acc: 0.7462 | Val Loss: 0.6955 - Val Acc: 0.7253\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 40/100 | Train Loss: 0.6308 - Train Acc: 0.7514 | Val Loss: 0.6588 - Val Acc: 0.7411\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 41/100 | Train Loss: 0.6247 - Train Acc: 0.7559 | Val Loss: 0.6644 - Val Acc: 0.7360\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 42/100 | Train Loss: 0.6215 - Train Acc: 0.7548 | Val Loss: 0.6818 - Val Acc: 0.7405\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 43/100 | Train Loss: 0.6198 - Train Acc: 0.7548 | Val Loss: 0.6651 - Val Acc: 0.7449\n",
      "Sin mejora. Contador de paciencia: 3 / 10\n",
      "Epoch 44/100 | Train Loss: 0.6056 - Train Acc: 0.7686 | Val Loss: 0.6633 - Val Acc: 0.7360\n",
      "Sin mejora. Contador de paciencia: 4 / 10\n",
      "Epoch 45/100 | Train Loss: 0.6000 - Train Acc: 0.7640 | Val Loss: 0.6819 - Val Acc: 0.7430\n",
      "Sin mejora. Contador de paciencia: 5 / 10\n",
      "Epoch 46/100 | Train Loss: 0.6008 - Train Acc: 0.7609 | Val Loss: 0.6444 - Val Acc: 0.7468\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 47/100 | Train Loss: 0.5905 - Train Acc: 0.7667 | Val Loss: 0.6516 - Val Acc: 0.7481\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 48/100 | Train Loss: 0.5773 - Train Acc: 0.7784 | Val Loss: 0.6582 - Val Acc: 0.7557\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 49/100 | Train Loss: 0.5703 - Train Acc: 0.7762 | Val Loss: 0.6465 - Val Acc: 0.7437\n",
      "Sin mejora. Contador de paciencia: 3 / 10\n",
      "Epoch 50/100 | Train Loss: 0.5623 - Train Acc: 0.7765 | Val Loss: 0.6819 - Val Acc: 0.7303\n",
      "Sin mejora. Contador de paciencia: 4 / 10\n",
      "Epoch 51/100 | Train Loss: 0.5656 - Train Acc: 0.7762 | Val Loss: 0.6319 - Val Acc: 0.7456\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 52/100 | Train Loss: 0.5543 - Train Acc: 0.7812 | Val Loss: 0.6513 - Val Acc: 0.7513\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 53/100 | Train Loss: 0.5462 - Train Acc: 0.7844 | Val Loss: 0.6462 - Val Acc: 0.7576\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 54/100 | Train Loss: 0.5475 - Train Acc: 0.7808 | Val Loss: 0.6588 - Val Acc: 0.7475\n",
      "Sin mejora. Contador de paciencia: 3 / 10\n",
      "Epoch 55/100 | Train Loss: 0.5450 - Train Acc: 0.7855 | Val Loss: 0.6551 - Val Acc: 0.7513\n",
      "Sin mejora. Contador de paciencia: 4 / 10\n",
      "Epoch 56/100 | Train Loss: 0.5304 - Train Acc: 0.7955 | Val Loss: 0.6493 - Val Acc: 0.7481\n",
      "Sin mejora. Contador de paciencia: 5 / 10\n",
      "Epoch 57/100 | Train Loss: 0.5320 - Train Acc: 0.7927 | Val Loss: 0.6807 - Val Acc: 0.7468\n",
      "Sin mejora. Contador de paciencia: 6 / 10\n",
      "Epoch 58/100 | Train Loss: 0.5209 - Train Acc: 0.7952 | Val Loss: 0.6995 - Val Acc: 0.7354\n",
      "Sin mejora. Contador de paciencia: 7 / 10\n",
      "Epoch 59/100 | Train Loss: 0.5162 - Train Acc: 0.7960 | Val Loss: 0.6450 - Val Acc: 0.7570\n",
      "Sin mejora. Contador de paciencia: 8 / 10\n",
      "Epoch 60/100 | Train Loss: 0.5023 - Train Acc: 0.8011 | Val Loss: 0.6670 - Val Acc: 0.7430\n",
      "Sin mejora. Contador de paciencia: 9 / 10\n",
      "Epoch 61/100 | Train Loss: 0.5042 - Train Acc: 0.8012 | Val Loss: 0.6628 - Val Acc: 0.7481\n",
      "Sin mejora. Contador de paciencia: 10 / 10\n",
      "--- EARLY STOPPING ACTIVADO ---\n",
      "Deteniendo el entrenamiento anticipadamente.\n",
      "--- Entrenamiento Finalizado ---\n",
      "Cargando el mejor modelo desde crnn_stumpy2.pth (Mejor Val Loss: 0.631933)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"9d6224b9-deb1-43a6-af8b-efa1bf54e1f5\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"9d6224b9-deb1-43a6-af8b-efa1bf54e1f5\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "indices = list(range(len(dataset)))\n",
    "labels_arr = [dataset.samples[i][1] for i in indices]\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=labels_arr, random_state=42)\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CRNN_Model(\n",
    "    n_channels_cnn=12,\n",
    "    rnn_hidden_size=128, \n",
    "    rnn_num_layers=2,    \n",
    "    num_classes=4,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopper = EarlyStopping(patience=10, mode='min', checkpoint_path='crnn_stumpy2.pth')\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "print(f\"--- Iniciando entrenamiento de CRNN en {device} ---\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for signals, labels, ids in train_loader:\n",
    "        signals = signals.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(signals) \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item() * signals.size(0)\n",
    "        total_train += signals.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        \n",
    "    train_acc = correct_train / total_train if total_train else 0.0\n",
    "    avg_train_loss = total_train_loss / total_train if total_train else 0.0\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val = 0\n",
    "    correct_val = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for signals, labels, ids in val_loader:\n",
    "            signals = signals.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(signals)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_val_loss += loss.item() * signals.size(0)\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_val += signals.size(0)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            \n",
    "    val_acc = correct_val / total_val if total_val else 0.0\n",
    "    avg_val_loss = total_val_loss / total_val if total_val else 0.0\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} - Train Acc: {train_acc:.4f} | Val Loss: {avg_val_loss:.4f} - Val Acc: {val_acc:.4f}')\n",
    "    early_stopper(avg_val_loss, model)\n",
    "    \n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Deteniendo el entrenamiento anticipadamente.\")\n",
    "        break\n",
    "\n",
    "print(\"--- Entrenamiento Finalizado ---\")\n",
    "\n",
    "print(f\"Cargando el mejor modelo desde {early_stopper.checkpoint_path} (Mejor Val Loss: {early_stopper.best_score:.6f})\")\n",
    "model.load_state_dict(torch.load(early_stopper.checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0484ef8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f100c7c4",
   "metadata": {},
   "source": [
    "Epoch 16/100 | Train Loss: 0.4743 - Train Acc: 0.8200 | Val Loss: 0.4341 - Val Acc: 0.8293\n",
    "Mejora detectada. Guardando modelo en crnn_best_model.pth\n",
    "\n",
    "\n",
    "\n",
    "Epoch 25/100 | Train Loss: 0.4435 - Train Acc: 0.8307 | Val Loss: 0.4710 - Val Acc: 0.8236\n",
    "Mejora detectada. Guardando modelo en crnn_stumpy.pth\n",
    "\n",
    "\n",
    "\n",
    "Epoch 51/100 | Train Loss: 0.5656 - Train Acc: 0.7762 | Val Loss: 0.6319 - Val Acc: 0.7456\n",
    "Mejora detectada. Guardando modelo en crnn_stumpy2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7815f5a2-2cd8-4d5c-a914-0a8fecdc111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Background signals shape: torch.Size([8, 12, 10000])\n",
      "Test signals shape: torch.Size([8, 12, 10000])\n",
      "Cargando modelo...\n",
      "Modelo cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap  # Necesitarás instalarlo: pip install shap\n",
    "from models.rnn import CRNN_Model\n",
    "\n",
    "# --- PARÁMETROS ---\n",
    "N_CHANNELS = 12  # <-- ¡Asegúrate que coincida con el modelo guardado!\n",
    "NUM_CLASSES = 4\n",
    "RNN_HIDDEN = 128\n",
    "RNN_LAYERS = 2\n",
    "MODEL_PATH = 'crnn_stumpy2.pth' # <-- El path de tu notebook\n",
    "DATA_FOLDER = 'data'\n",
    "LEADS = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "mp = ['mp_' + i for i in LEADS]\n",
    "# LEADS = LEADS + mp\n",
    "# print(LEADS)\n",
    "\n",
    "CLASS_FOLDERS = {\n",
    "    'arritmia': 0,\n",
    "    'block': 1,\n",
    "    'fibrilation': 2,\n",
    "    'normal': 3\n",
    "}\n",
    "\n",
    "# --- 1. Cargar Datos para SHAP ---\n",
    "print(\"Cargando datos...\")\n",
    "# Usar pocos archivos por clase para SHAP, es más rápido\n",
    "# Necesitamos datos de fondo (entrenamiento) y datos de prueba (validación)\n",
    "dataset = ECGDataset(DATA_FOLDER, CLASS_FOLDERS, files_per_class=50) \n",
    "indices = list(range(len(dataset)))\n",
    "labels_arr = [dataset.samples[i][1] for i in indices]\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.5, stratify=labels_arr, random_state=42)\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "# SHAP necesita lotes de datos\n",
    "# Un lote de fondo (background) y un lote de prueba (test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False) # Lote más pequeño para explicar\n",
    "\n",
    "# Tomar un lote de fondo (para el 'baseline' de SHAP)\n",
    "background_signals, _, _ = next(iter(train_loader))\n",
    "# Tomar un lote de prueba (los que queremos explicar)\n",
    "test_signals, test_labels, _ = next(iter(val_loader))\n",
    "\n",
    "print(f\"Background signals shape: {background_signals.shape}\")\n",
    "print(f\"Test signals shape: {test_signals.shape}\")\n",
    "\n",
    "# --- 2. Cargar Modelo ---\n",
    "print(\"Cargando modelo...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CRNN_Model(\n",
    "    n_channels_cnn=N_CHANNELS,\n",
    "    rnn_hidden_size=RNN_HIDDEN,\n",
    "    rnn_num_layers=RNN_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "except RuntimeError as e:\n",
    "    print(f\"--- ¡ERROR AL CARGAR EL MODELO! ---\")\n",
    "    print(e)\n",
    "    print(\"\\nEsto suele pasar si 'N_CHANNELS' no coincide con el modelo guardado.\")\n",
    "    print(f\"Estás intentando cargar {N_CHANNELS} canales.\")\n",
    "    print(\"Verifica el 'n_channels_cnn' en tu notebook (era 13) vs los leads de 'ECGDataset' (eran 12).\")\n",
    "    print(\"Ajusta 'N_CHANNELS' en este script para que coincida con el modelo .pth guardado.\")\n",
    "    exit() # Salir si no se puede cargar\n",
    "\n",
    "model.eval()\n",
    "print(\"Modelo cargado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b993951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando SHAP values...\n",
      "Cálculo de SHAP finalizado.\n",
      "SHAP values tiene 8 elementos (uno por clase)\n",
      "El shape de los SHAP para la clase 0 es: (12, 10000, 4)\n",
      "\n",
      "--- Importancia Absoluta Media por Lead ---\n",
      "mp_V2: 0.000274\n",
      "mp_aVR: 0.000271\n",
      "mp_aVF: 0.000269\n",
      "mp_II: 0.000254\n",
      "mp_I: 0.000246\n",
      "mp_V1: 0.000244\n",
      "mp_V4: 0.000238\n",
      "mp_V3: 0.000232\n",
      "mp_aVL: 0.000221\n",
      "mp_III: 0.000220\n",
      "mp_V6: 0.000143\n",
      "mp_V5: 0.000100\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Calcular SHAP Values ---\n",
    "print(\"Calculando SHAP values...\")\n",
    "\n",
    "background_signals = background_signals.to(device)\n",
    "test_signals = test_signals.to(device)\n",
    "\n",
    "# Desactivar CuDNN fuerza a usar la implementación nativa de PyTorch (más lenta pero flexible)\n",
    "torch.backends.cudnn.enabled = False \n",
    "\n",
    "try:\n",
    "    # Aquí puedes dejar el modelo en eval() o train(), usualmente eval() funciona sin CuDNN\n",
    "    model.eval() \n",
    "    \n",
    "    explainer = shap.GradientExplainer(model, background_signals)\n",
    "    shap_values = explainer.shap_values(test_signals)\n",
    "    \n",
    "finally:\n",
    "    # ¡Muy importante reactivarlo al terminar!\n",
    "    torch.backends.cudnn.enabled = True \n",
    "\n",
    "print(\"Cálculo de SHAP finalizado.\")\n",
    "\n",
    "# shap_values es una lista (una por clase) de arrays\n",
    "# Cada array tiene la forma (batch_size, 12, 10000)\n",
    "print(f\"SHAP values tiene {len(shap_values)} elementos (uno por clase)\")\n",
    "print(f\"El shape de los SHAP para la clase 0 es: {shap_values[0].shape}\")\n",
    "\n",
    "\n",
    "# --- 4. Analizar Importancia por Lead ---\n",
    "print(\"\\n--- Importancia Absoluta Media por Lead ---\")\n",
    "\n",
    "# Para obtener la importancia general de cada lead, promediamos el valor absoluto\n",
    "# de SHAP a través de todas las clases, muestras y tiempo.\n",
    "# Convertimos la lista de (N_samples, N_leads, N_time) a (N_classes, N_samples, N_leads, N_time)\n",
    "shap_values_np = np.array(shap_values)\n",
    "\n",
    "# (N_classes, N_samples, N_leads, N_time) -> (N_leads)\n",
    "# Promediamos sobre clases, muestras y tiempo\n",
    "mean_abs_shap = np.mean(np.abs(shap_values_np), axis=(0, 1, 3))\n",
    "\n",
    "lead_importance = sorted(zip(mp, mean_abs_shap), key=lambda x: x[1], reverse=True)\n",
    "for lead, importance in lead_importance:\n",
    "    print(f\"{lead}: {importance:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c4aa9",
   "metadata": {},
   "source": [
    "--- Importancia Absoluta Media por Lead ---\n",
    "aVL: 0.000084\n",
    "V2: 0.000084\n",
    "V4: 0.000077\n",
    "V3: 0.000071\n",
    "V6: 0.000070\n",
    "aVF: 0.000066\n",
    "aVR: 0.000066\n",
    "II: 0.000065\n",
    "V1: 0.000064\n",
    "V5: 0.000062\n",
    "III: 0.000060\n",
    "I: 0.000057"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5002be2b",
   "metadata": {},
   "source": [
    "Cargando datos...\n",
    "Background signals shape: torch.Size([8, 24, 10000])\n",
    "Test signals shape: torch.Size([8, 24, 10000])\n",
    "Cargando modelo...\n",
    "Modelo cargado exitosamente.\n",
    "Calculando SHAP values (esto puede tardar)...\n",
    "Cálculo de SHAP finalizado.\n",
    "SHAP values tiene 8 elementos (uno por clase)\n",
    "El shape de los SHAP para la clase 0 es: (24, 10000, 4)\n",
    "\n",
    "#### --- Importancia Absoluta Media por Lead ---\n",
    "- mp_aVR: 0.000084\n",
    "- mp_V2: 0.000082\n",
    "- V2: 0.000081\n",
    "- mp_V6: 0.000080\n",
    "- mp_aVF: 0.000080\n",
    "- V6: 0.000076\n",
    "- mp_V4: 0.000074\n",
    "- aVR: 0.000074\n",
    "- mp_V1: 0.000072\n",
    "- mp_aVL: 0.000070\n",
    "- V4: 0.000069\n",
    "- mp_II: 0.000069\n",
    "- mp_V3: 0.000066\n",
    "- mp_III: 0.000065\n",
    "- mp_V5: 0.000063\n",
    "- aVF: 0.000060\n",
    "- V3: 0.000060\n",
    "- mp_I: 0.000060\n",
    "- aVL: 0.000059\n",
    "- II: 0.000058\n",
    "- V1: 0.000057\n",
    "- III: 0.000057\n",
    "- I: 0.000054\n",
    "- V5: 0.000048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3548b39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df0f2d69-7073-4af0-beee-58ee6b71f214",
   "metadata": {},
   "source": [
    "Calculando SHAP values...\n",
    "Cálculo de SHAP finalizado.\n",
    "SHAP values tiene 8 elementos (uno por clase)\n",
    "El shape de los SHAP para la clase 0 es: (12, 10000, 4)\n",
    "\n",
    "### --- Importancia Absoluta Media por Lead ---\n",
    "- mp_V2: 0.000274\n",
    "- mp_aVR: 0.000271\n",
    "- mp_aVF: 0.000269\n",
    "- mp_II: 0.000254\n",
    "- mp_I: 0.000246\n",
    "- mp_V1: 0.000244\n",
    "- mp_V4: 0.000238\n",
    "mp_V3: 0.000232\n",
    "mp_aVL: 0.000221\n",
    "mp_III: 0.000220\n",
    "mp_V6: 0.000143\n",
    "mp_V5: 0.000100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
