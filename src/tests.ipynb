{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9870c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.rnn import CRNN_Model\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "from neurokit2 import ecg\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import stumpy\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SAMPLING_RATE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edff6a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "if (!(\"Notification\" in window)) {\n    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n    Notification.requestPermission(function (permission) {\n        if(!('permission' in Notification)) {\n            Notification.permission = permission;\n        }\n    })\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5387c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signals shape: torch.Size([8, 24, 10000])\n",
      "Labels: tensor([1, 1, 2, 3, 3, 2, 1, 1])\n",
      "ECG IDs: ('454197.parquet.gzip', '267480.parquet.gzip', '542910.parquet.gzip', '376298.parquet.gzip', '81699.parquet.gzip', '497856.parquet.gzip', '85590.parquet.gzip', '264503.parquet.gzip')\n"
     ]
    }
   ],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, data_folder, class_folders, files_per_class=200, mp_window = 5000):\n",
    "        self.samples = []\n",
    "        self.leads = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "        self.mp_window = mp_window\n",
    "        for folder, label in class_folders.items():\n",
    "            files = glob.glob(os.path.join(data_folder, folder, '*.parquet.gzip'))\n",
    "            if len(files) >= files_per_class:\n",
    "                files = random.sample(files, files_per_class)\n",
    "            else:\n",
    "                files = random.choices(files, k=files_per_class)\n",
    "\n",
    "            for f in files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(f, engine='fastparquet')\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read {f}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # ensure required lead columns exist\n",
    "                if not set(self.leads).issubset(df.columns):\n",
    "                    print(f\"Missing leads in {f}, skipping\")\n",
    "                    continue\n",
    "\n",
    "                # convert lead columns to numeric, coerce non-numeric to NaN, then fill and cast\n",
    "                df_leads = df[self.leads].apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n",
    "\n",
    "                # shape -> (12, time)\n",
    "                signal = df_leads.values.T\n",
    "\n",
    "                matrix_profiles = []\n",
    "                for i in range(signal.shape[0]):\n",
    "                    mp = stumpy.stump(signal[i].astype(np.float64), m=self.mp_window)[:, 0].astype(np.float32)\n",
    "                    pad_width = signal.shape[1] - len(mp)\n",
    "                    padded_mp = np.pad(mp, (0, pad_width), 'constant', constant_values=0)\n",
    "                    padded_mp[np.isinf(padded_mp)] = 1e9\n",
    "                    matrix_profiles.append(padded_mp)\n",
    "                matrix_profiles_np = np.array(matrix_profiles, dtype=np.float32)\n",
    "                combined_signal = np.concatenate((signal, matrix_profiles_np), axis=0)\n",
    "                self.samples.append((torch.tensor(combined_signal, dtype=torch.float32), label, os.path.basename(f)))\n",
    "                # self.samples.append((torch.tensor(matrix_profiles_np, dtype=torch.float32), label, os.path.basename(f)))\n",
    "                # self.samples.append((torch.tensor(signal, dtype=torch.float32), label, os.path.basename(f)))\n",
    "    def process_file(self, f, label):\n",
    "        try:\n",
    "            df = pd.read_parquet(f, engine='fastparquet')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {f}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        if not set(self.leads).issubset(df.columns):\n",
    "            print(f\"Missing leads in {f}, skipping\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal, label, ecg_id = self.samples[idx]\n",
    "        return signal, label, ecg_id\n",
    "\n",
    "# Usage example\n",
    "class_folders = {\n",
    "    'arritmia': 0,\n",
    "    'block': 1,\n",
    "    'fibrilation': 2,\n",
    "    'normal': 3\n",
    "}\n",
    "data_folder = 'data'\n",
    "dataset = ECGDataset(data_folder, class_folders, files_per_class=1970)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Inspect one batch\n",
    "for signals, labels, ecg_ids in dataloader:\n",
    "    print('Signals shape:', signals.shape)  # (batch, 12, time)\n",
    "    print('Labels:', labels)\n",
    "    print('ECG IDs:', ecg_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7abe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 40 samples + split indices to loader12MP_CRNN.pth\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(len(dataset)))\n",
    "labels_arr = [dataset.samples[i][1] for i in indices]\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=labels_arr, random_state=42)\n",
    "\n",
    "serializable = {\n",
    "    'samples': [(sig.cpu().numpy(), int(label), str(ecg_id)) for sig, label, ecg_id in dataset.samples],\n",
    "    'train_idx': train_idx,\n",
    "    'val_idx': val_idx,\n",
    "    'leads': dataset.leads,\n",
    "    'mp_window': dataset.mp_window\n",
    "}\n",
    "torch.save(serializable, 'loader12MP_CRNN.pth')\n",
    "print(f\"Saved {len(serializable['samples'])} samples + split indices to loader12MP_CRNN.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fbb47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data from loader12MP_CRNN.pth...\n",
      "Loaded 40 total samples.\n",
      "Training samples: 32, Validation samples: 8\n",
      "\n",
      "DataLoaders created successfully.\n",
      "Sample batch shape: torch.Size([8, 12, 10000])\n",
      "Sample batch labels: tensor([3, 3, 2, 2, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "class PreloadedECGDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        # Convert numpy arrays back to tensors\n",
    "        self.samples = [(torch.tensor(sig, dtype=torch.float32), label, ecg_id) for sig, label, ecg_id in samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# --- Load the pre-processed data ---\n",
    "print(\"Loading pre-processed data from loader12MP_CRNN.pth...\")\n",
    "loaded_data = torch.load('loader12MP_CRNN.pth', weights_only=False)\n",
    "\n",
    "# Extract the components\n",
    "all_samples = loaded_data['samples']\n",
    "train_idx = loaded_data['train_idx']\n",
    "val_idx = loaded_data['val_idx']\n",
    "\n",
    "print(f\"Loaded {len(all_samples)} total samples.\")\n",
    "print(f\"Training samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "# Create a full dataset object from the loaded samples\n",
    "full_dataset = PreloadedECGDataset(all_samples)\n",
    "\n",
    "# Create subsets for training and validation using the saved indices\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(full_dataset, val_idx)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"\\nDataLoaders created successfully.\")\n",
    "\n",
    "# Optional: Inspect a batch to verify\n",
    "signals, labels, ids = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {signals.shape}\")\n",
    "print(f\"Sample batch labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d5cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0, mode='min', checkpoint_path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.counter = 0\n",
    "        self.best_score = np.inf if mode == 'min' else -np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score, model):\n",
    "        is_better = False\n",
    "        if self.mode == 'min':\n",
    "            is_better = current_score < (self.best_score - self.min_delta)\n",
    "        else:\n",
    "            is_better = current_score > (self.best_score + self.min_delta)\n",
    "\n",
    "        if is_better:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            print(f\"Mejora detectada. Guardando modelo en {self.checkpoint_path}\")\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"Sin mejora. Contador de paciencia: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(\"--- EARLY STOPPING ACTIVADO ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f62b31",
   "metadata": {},
   "source": [
    "ECG eliminados por peso (2kb):\n",
    "\n",
    "- block: 8846, 314864\n",
    "- normal: 74424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando entrenamiento de CRNN en cpu ---\n",
      "Epoch 1/100 | Train Loss: 1.0965 - Train Acc: 0.5114 | Val Loss: 0.6927 - Val Acc: 0.7557\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 2/100 | Train Loss: 0.7054 - Train Acc: 0.7314 | Val Loss: 0.5689 - Val Acc: 0.8008\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 3/100 | Train Loss: 0.6463 - Train Acc: 0.7582 | Val Loss: 0.5838 - Val Acc: 0.7862\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 4/100 | Train Loss: 0.5777 - Train Acc: 0.7878 | Val Loss: 0.6158 - Val Acc: 0.7754\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 5/100 | Train Loss: 0.5410 - Train Acc: 0.7943 | Val Loss: 0.4952 - Val Acc: 0.8179\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 6/100 | Train Loss: 0.5306 - Train Acc: 0.8003 | Val Loss: 0.4968 - Val Acc: 0.8103\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 7/100 | Train Loss: 0.6233 - Train Acc: 0.7638 | Val Loss: 0.5416 - Val Acc: 0.7989\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 8/100 | Train Loss: 0.5301 - Train Acc: 0.7966 | Val Loss: 0.5022 - Val Acc: 0.8103\n",
      "Sin mejora. Contador de paciencia: 3 / 10\n",
      "Epoch 9/100 | Train Loss: 0.5019 - Train Acc: 0.8046 | Val Loss: 0.5293 - Val Acc: 0.8141\n",
      "Sin mejora. Contador de paciencia: 4 / 10\n",
      "Epoch 10/100 | Train Loss: 0.4995 - Train Acc: 0.8103 | Val Loss: 0.4957 - Val Acc: 0.8230\n",
      "Sin mejora. Contador de paciencia: 5 / 10\n",
      "Epoch 11/100 | Train Loss: 0.4744 - Train Acc: 0.8146 | Val Loss: 0.5170 - Val Acc: 0.8160\n",
      "Sin mejora. Contador de paciencia: 6 / 10\n",
      "Epoch 12/100 | Train Loss: 0.4744 - Train Acc: 0.8127 | Val Loss: 0.4796 - Val Acc: 0.8280\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 13/100 | Train Loss: 0.4783 - Train Acc: 0.8171 | Val Loss: 0.5091 - Val Acc: 0.8160\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 14/100 | Train Loss: 0.4632 - Train Acc: 0.8185 | Val Loss: 0.4915 - Val Acc: 0.8173\n",
      "Sin mejora. Contador de paciencia: 2 / 10\n",
      "Epoch 15/100 | Train Loss: 0.4731 - Train Acc: 0.8149 | Val Loss: 0.4800 - Val Acc: 0.8236\n",
      "Sin mejora. Contador de paciencia: 3 / 10\n",
      "Epoch 16/100 | Train Loss: 0.4686 - Train Acc: 0.8184 | Val Loss: 0.4926 - Val Acc: 0.8217\n",
      "Sin mejora. Contador de paciencia: 4 / 10\n",
      "Epoch 17/100 | Train Loss: 0.4585 - Train Acc: 0.8265 | Val Loss: 0.5074 - Val Acc: 0.8046\n",
      "Sin mejora. Contador de paciencia: 5 / 10\n",
      "Epoch 18/100 | Train Loss: 0.4641 - Train Acc: 0.8247 | Val Loss: 0.4939 - Val Acc: 0.8065\n",
      "Sin mejora. Contador de paciencia: 6 / 10\n",
      "Epoch 19/100 | Train Loss: 0.4435 - Train Acc: 0.8239 | Val Loss: 0.4783 - Val Acc: 0.8287\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 20/100 | Train Loss: 0.4310 - Train Acc: 0.8282 | Val Loss: 0.4706 - Val Acc: 0.8223\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 21/100 | Train Loss: 0.4288 - Train Acc: 0.8344 | Val Loss: 0.4804 - Val Acc: 0.8192\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n",
      "Epoch 22/100 | Train Loss: 0.4213 - Train Acc: 0.8372 | Val Loss: 0.4553 - Val Acc: 0.8376\n",
      "Mejora detectada. Guardando modelo en crnn_stumpy2.pth\n",
      "Epoch 23/100 | Train Loss: 0.4058 - Train Acc: 0.8442 | Val Loss: 0.4682 - Val Acc: 0.8268\n",
      "Sin mejora. Contador de paciencia: 1 / 10\n"
     ]
    }
   ],
   "source": [
    "%%notify\n",
    "indices = list(range(len(dataset)))\n",
    "labels_arr = [dataset.samples[i][1] for i in indices]\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=labels_arr, random_state=42)\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CRNN_Model(\n",
    "    n_channels_cnn=24,\n",
    "    rnn_hidden_size=128, \n",
    "    rnn_num_layers=2,    \n",
    "    num_classes=4,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopper = EarlyStopping(patience=10, mode='min', checkpoint_path='crnn_stumpy2.pth')\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "print(f\"--- Iniciando entrenamiento de CRNN en {device} ---\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    \n",
    "    for signals, labels, ids in train_loader:\n",
    "        signals = signals.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(signals) \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item() * signals.size(0)\n",
    "        total_train += signals.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        \n",
    "    train_acc = correct_train / total_train if total_train else 0.0\n",
    "    avg_train_loss = total_train_loss / total_train if total_train else 0.0\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val = 0\n",
    "    correct_val = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for signals, labels, ids in val_loader:\n",
    "            signals = signals.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(signals)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_val_loss += loss.item() * signals.size(0)\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_val += signals.size(0)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            \n",
    "    val_acc = correct_val / total_val if total_val else 0.0\n",
    "    avg_val_loss = total_val_loss / total_val if total_val else 0.0\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} - Train Acc: {train_acc:.4f} | Val Loss: {avg_val_loss:.4f} - Val Acc: {val_acc:.4f}')\n",
    "    early_stopper(avg_val_loss, model)\n",
    "    \n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Deteniendo el entrenamiento anticipadamente.\")\n",
    "        break\n",
    "\n",
    "print(\"--- Entrenamiento Finalizado ---\")\n",
    "\n",
    "print(f\"Cargando el mejor modelo desde {early_stopper.checkpoint_path} (Mejor Val Loss: {early_stopper.best_score:.6f})\")\n",
    "model.load_state_dict(torch.load(early_stopper.checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100c7c4",
   "metadata": {},
   "source": [
    "Epoch 16/100 | Train Loss: 0.4743 - Train Acc: 0.8200 | Val Loss: 0.4341 - Val Acc: 0.8293\n",
    "Mejora detectada. Guardando modelo en crnn_best_model.pth (12 leads)\n",
    "\n",
    "\n",
    "\n",
    "Epoch 25/100 | Train Loss: 0.4435 - Train Acc: 0.8307 | Val Loss: 0.4710 - Val Acc: 0.8236\n",
    "Mejora detectada. Guardando modelo en crnn_stumpy.pth (12 leads + 12 mp, 1s)\n",
    "\n",
    "\n",
    "\n",
    "Epoch 51/100 | Train Loss: 0.5656 - Train Acc: 0.7762 | Val Loss: 0.6319 - Val Acc: 0.7456\n",
    "Mejora detectada. Guardando modelo en crnn_stumpy2.pth (12 mp)\n",
    "\n",
    "\n",
    "Epoch 22/100 | Train Loss: 0.4213 - Train Acc: 0.8372 | Val Loss: 0.4553 - Val Acc: 0.8376\n",
    "Mejora detectada. Guardando modelo en crnn_stumpy2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7815f5a2-2cd8-4d5c-a914-0a8fecdc111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Background signals shape: torch.Size([8, 12, 10000])\n",
      "Test signals shape: torch.Size([8, 12, 10000])\n",
      "Cargando modelo...\n",
      "Modelo cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from models.rnn import CRNN_Model\n",
    "\n",
    "N_CHANNELS = 12\n",
    "NUM_CLASSES = 4\n",
    "RNN_HIDDEN = 128\n",
    "RNN_LAYERS = 2\n",
    "MODEL_PATH = 'crnn_stumpy3.pth'\n",
    "DATA_FOLDER = 'data'\n",
    "LEADS = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "mp = ['mp_' + i for i in LEADS]\n",
    "\n",
    "CLASS_FOLDERS = {\n",
    "    'arritmia': 0,\n",
    "    'block': 1,\n",
    "    'fibrilation': 2,\n",
    "    'normal': 3\n",
    "}\n",
    "\n",
    "print(\"Cargando datos...\")\n",
    "\n",
    "dataset = ECGDataset(DATA_FOLDER, CLASS_FOLDERS, files_per_class=50) \n",
    "indices = list(range(len(dataset)))\n",
    "labels_arr = [dataset.samples[i][1] for i in indices]\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.5, stratify=labels_arr, random_state=42)\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "background_signals, _, _ = next(iter(train_loader))\n",
    "\n",
    "test_signals, test_labels, _ = next(iter(val_loader))\n",
    "\n",
    "print(f\"Background signals shape: {background_signals.shape}\")\n",
    "print(f\"Test signals shape: {test_signals.shape}\")\n",
    "\n",
    "print(\"Cargando modelo...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CRNN_Model(\n",
    "    n_channels_cnn=N_CHANNELS,\n",
    "    rnn_hidden_size=RNN_HIDDEN,\n",
    "    rnn_num_layers=RNN_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "except RuntimeError as e:\n",
    "    print(f\"--- ¡ERROR AL CARGAR EL MODELO! ---\")\n",
    "    print(e)\n",
    "    print(\"\\nEsto suele pasar si 'N_CHANNELS' no coincide con el modelo guardado.\")\n",
    "    print(f\"Estás intentando cargar {N_CHANNELS} canales.\")\n",
    "    print(\"Verifica el 'n_channels_cnn' en tu notebook (era 13) vs los leads de 'ECGDataset' (eran 12).\")\n",
    "    print(\"Ajusta 'N_CHANNELS' en este script para que coincida con el modelo .pth guardado.\")\n",
    "    exit()\n",
    "\n",
    "model.eval()\n",
    "print(\"Modelo cargado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b993951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando SHAP values...\n",
      "Cálculo de SHAP finalizado.\n",
      "SHAP values tiene 8 elementos (uno por clase)\n",
      "El shape de los SHAP para la clase 0 es: (12, 10000, 4)\n",
      "\n",
      "--- Importancia Absoluta Media por Lead ---\n",
      "mp_V2: 0.000274\n",
      "mp_aVR: 0.000271\n",
      "mp_aVF: 0.000269\n",
      "mp_II: 0.000254\n",
      "mp_I: 0.000246\n",
      "mp_V1: 0.000244\n",
      "mp_V4: 0.000238\n",
      "mp_V3: 0.000232\n",
      "mp_aVL: 0.000221\n",
      "mp_III: 0.000220\n",
      "mp_V6: 0.000143\n",
      "mp_V5: 0.000100\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculando SHAP values...\")\n",
    "\n",
    "background_signals = background_signals.to(device)\n",
    "test_signals = test_signals.to(device)\n",
    "\n",
    "torch.backends.cudnn.enabled = False \n",
    "try:\n",
    "    model.eval() \n",
    "    \n",
    "    explainer = shap.GradientExplainer(model, background_signals)\n",
    "    shap_values = explainer.shap_values(test_signals)\n",
    "    \n",
    "finally:\n",
    "    torch.backends.cudnn.enabled = True \n",
    "\n",
    "# --- 4. Analizar Importancia por Lead ---\n",
    "print(\"\\n--- Importancia Absoluta Media por Lead ---\")\n",
    "shap_values_np = np.array(shap_values)\n",
    "mean_abs_shap = np.mean(np.abs(shap_values_np), axis=(0, 1, 3))\n",
    "\n",
    "lead_importance = sorted(zip(mp, mean_abs_shap), key=lambda x: x[1], reverse=True)\n",
    "for lead, importance in lead_importance:\n",
    "    print(f\"{lead}: {importance:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c4aa9",
   "metadata": {},
   "source": [
    "#### --- Importancia Absoluta Media por Lead ---\n",
    "- aVL: 0.000084\n",
    "- V2: 0.000084\n",
    "- V4: 0.000077\n",
    "- V3: 0.000071\n",
    "- V6: 0.000070\n",
    "- aVF: 0.000066\n",
    "- aVR: 0.000066\n",
    "- II: 0.000065\n",
    "- V1: 0.000064\n",
    "- V5: 0.000062\n",
    "- III: 0.000060\n",
    "- I: 0.000057"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5002be2b",
   "metadata": {},
   "source": [
    "#### --- Importancia Absoluta Media por Lead ---\n",
    "- mp_aVR: 0.000084\n",
    "- mp_V2: 0.000082\n",
    "- V2: 0.000081\n",
    "- mp_V6: 0.000080\n",
    "- mp_aVF: 0.000080\n",
    "- V6: 0.000076\n",
    "- mp_V4: 0.000074\n",
    "- aVR: 0.000074\n",
    "- mp_V1: 0.000072\n",
    "- mp_aVL: 0.000070\n",
    "- V4: 0.000069\n",
    "- mp_II: 0.000069\n",
    "- mp_V3: 0.000066\n",
    "- mp_III: 0.000065\n",
    "- mp_V5: 0.000063\n",
    "- aVF: 0.000060\n",
    "- V3: 0.000060\n",
    "- mp_I: 0.000060\n",
    "- aVL: 0.000059\n",
    "- II: 0.000058\n",
    "- V1: 0.000057\n",
    "- III: 0.000057\n",
    "- I: 0.000054\n",
    "- V5: 0.000048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3548b39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df0f2d69-7073-4af0-beee-58ee6b71f214",
   "metadata": {},
   "source": [
    "SOLO MP\n",
    "\n",
    "### --- Importancia Absoluta Media por Lead ---\n",
    "- mp_V2: 0.000274\n",
    "- mp_aVR: 0.000271\n",
    "- mp_aVF: 0.000269\n",
    "- mp_II: 0.000254\n",
    "- mp_I: 0.000246\n",
    "- mp_V1: 0.000244\n",
    "- mp_V4: 0.000238\n",
    "- mp_V3: 0.000232\n",
    "- mp_aVL: 0.000221\n",
    "- mp_III: 0.000220\n",
    "- mp_V6: 0.000143\n",
    "- mp_V5: 0.000100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de6e09",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
